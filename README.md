# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fQ2J771J6jYhffq-f78_TeoWY9ZBVP4s
"""

<!-- Chat Section -->
<section class="chat-section">
    <h2>Ask the Assistant</h2>
    <form action="/ask" method="POST">
        <label for="question">Ask a Question: </label>
        <input type="text" id="question" name="question" required />
        <button type="submit">Submit</button>
    </form>
    {% if question_response %}
    <div class="response">
        <h3>Response: </h3>
        <div class="response-text">
            {{ question_response | safe }}
        </div>
    </div>
    {% endif %}
</section>
<!-- Sentiment Analysis Section -->
<section class="sentiment-section">
    <h2>Feedback Sentiment</h2>
    <form action="/feedback" method="POST">
        <label for="feedback">Enter Feedback for Sentiment:</label>
        <textarea id="feedback" name="feedback" rows="4"></textarea>
        <button type="submit">Submit</button>
    </form>

    {% if sentiment %}
    <div class="response">
        <h3>Sentiment:</h3>
        <p>{{ sentiment }}</p>
    </div>
    {% endif %}
</section>
<!-- Concern Submission Section -->
<section class="concern-section">
    <h2>Report a Concern</h2>
    <form action="/concern" method="POST">
        <label for="concern">Describe Your Concern (for Issue Identification): </label>
        <textarea id="concern" name="concern" rows="4"></textarea>
        <button type="submit">Submit</button>
    </form>
    {% if concern_submitted %}
    <div class="response">
        <h3>Concern Submitted:</h3>
        <p>Your concern has been recorded.</p>
    </div>
    {% endif %}
</section>
</main>
</body>
</html>
# -------------------- Model Setup --------------------
# Note: Running large models locally requires significant resources (GPU, RAM)
model_path = "ibm-granite/granite-3.3-8b-instruct"

# Determine the device to use (GPU if available, otherwise CPU)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Load the tokenizer first
tokenizer = AutoTokenizer.from_pretrained(model_path)# IBM-granite
